{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part3-Zero shot transfer learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNkax+aT7CPWDH5JNdiYo5R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyUIWjzOi23X","outputId":"42c71a78-d5ce-46a6-85f5-d35d37be7b08","executionInfo":{"status":"ok","timestamp":1651967886095,"user_tz":420,"elapsed":3617,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["#installing some dependencies, CLIP was release in PyTorch\n","import subprocess\n","\n","CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n","print(\"CUDA version:\", CUDA_version)\n","\n","if CUDA_version == \"10.0\":\n","    torch_version_suffix = \"+cu100\"\n","elif CUDA_version == \"10.1\":\n","    torch_version_suffix = \"+cu101\"\n","elif CUDA_version == \"10.2\":\n","    torch_version_suffix = \"\"\n","else:\n","    torch_version_suffix = \"+cu110\"\n","\n","# !pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n","\n","import numpy as np\n","import torch\n","import os\n","\n","# print(\"Torch version:\", torch.__version__)\n","# os.kill(os.getpid(), 9)\n","#Your notebook process will restart after these installs"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA version: 11.1\n"]}]},{"cell_type":"code","metadata":{"id":"cqN0UVpssA7J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3b92b60-225b-44a4-b3a6-9bc578c2f600","executionInfo":{"status":"ok","timestamp":1651967889791,"user_tz":420,"elapsed":1205,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["#clone the CLIP repository\n","!git clone https://github.com/openai/CLIP.git\n","%cd CLIP"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 222, done.\u001b[K\n","remote: Total 222 (delta 0), reused 0 (delta 0), pack-reused 222\u001b[K\n","Receiving objects: 100% (222/222), 8.91 MiB | 30.12 MiB/s, done.\n","Resolving deltas: 100% (113/113), done.\n","/content/CLIP\n"]}]},{"cell_type":"code","metadata":{"id":"JWy5SOmV5tLK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc041299-7646-4779-b7fb-40074a617609","executionInfo":{"status":"ok","timestamp":1651967901953,"user_tz":420,"elapsed":10968,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["#follow the link below to get your download code from from Roboflow\n","!pip install -q roboflow\n","from roboflow import Roboflow\n","rf = Roboflow(model_format=\"clip\", notebook=\"roboflow-clip\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |██▎                             | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 145 kB 5.1 MB/s \n","\u001b[K     |████████████████████████████████| 178 kB 44.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 56.7 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 7.4 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n","\u001b[K     |████████████████████████████████| 138 kB 71.1 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 52.0 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25h  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=clip&ref=roboflow-clip\n"]}]},{"cell_type":"code","metadata":{"id":"AHHnCdsKrCVF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fc60b44-0ed9-4f0a-b6a1-fc03b5591c44","executionInfo":{"status":"ok","timestamp":1651967937045,"user_tz":420,"elapsed":24224,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["# download classification data\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"mkitKptEErYUTmUh8DjM\")\n","project = rf.workspace().project(\"flowers\")\n","dataset = project.version(\"1\").download(\"clip\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in Flowers-1 to clip: 100% [63963439 / 63963439] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to Flowers-1 in clip:: 100%|██████████| 1827/1827 [00:01<00:00, 990.78it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"sFsApb_r65Us","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"827ce4b0-3c2e-43ee-e436-9d55ec165702","executionInfo":{"status":"ok","timestamp":1651968024813,"user_tz":420,"elapsed":161,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["dataset.location"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/CLIP/Flowers-1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"-m7FYTSjCN46","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3d99073b-f490-4f8e-b6fb-fec191c2ce96","executionInfo":{"status":"ok","timestamp":1651968025976,"user_tz":420,"elapsed":140,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["import os\n","#our the classes and images we want to test are stored in folders in the test set\n","class_names = os.listdir('/content/CLIP/tests/')\n","# class_names.remove('_tokenization.txt')\n","class_names"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['test_consistency.py']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"QELzUB7pnr-h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a388c1dd-da4c-4a06-8a73-2ab420c8b792","executionInfo":{"status":"ok","timestamp":1651968027883,"user_tz":420,"elapsed":153,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["#we auto generate some example tokenizations in Roboflow but you should edit this file to try out your own prompts\n","#CLIP gets a lot better with the right prompting!\n","#be sure the tokenizations are in the same order as your class_names above!\n","%cat {dataset.location}/test/_tokenization.txt"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["cat: /content/CLIP/Flowers-1/test/_tokenization.txt: No such file or directory\n"]}]},{"cell_type":"code","metadata":{"id":"jPQxIGxXn8bR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8d6a92d-1ed1-42f7-edb1-126965583fca","executionInfo":{"status":"ok","timestamp":1651968029760,"user_tz":420,"elapsed":134,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["#edit your prompts as you see fit here, be sure the classes are in teh same order as above\n","%%writefile /content/CLIP/Flowers-1/train/_tokenization.txt\n","The paper sign in rock paper scissors\n","The rock sign in rock paper scissors\n","The scissors sign in rock paper scissors"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/CLIP/Flowers-1/train/_tokenization.txt\n"]}]},{"cell_type":"code","metadata":{"id":"gpXaqPH3oSyO","executionInfo":{"status":"ok","timestamp":1651968030840,"user_tz":420,"elapsed":142,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}}},"source":["candidate_captions = []\n","with open('/content/CLIP/Flowers-1/train/_tokenization.txt') as f:\n","    candidate_captions = f.read().splitlines()"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0Toc3jn-AzL","executionInfo":{"status":"ok","timestamp":1651968075552,"user_tz":420,"elapsed":2688,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}},"outputId":"f494297f-699b-451a-d1bd-75a020aab182"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n"]}]},{"cell_type":"code","metadata":{"id":"yAi7cvucnFPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651968102303,"user_tz":420,"elapsed":24244,"user":{"displayName":"Anusha Gangasani","userId":"04289121292962379900"}},"outputId":"f2a21bc1-04a7-49e0-c16c-69edd59b42ef"},"source":["import torch\n","import clip\n","from PIL import Image\n","import glob\n","\n","def argmax(iterable):\n","    return max(enumerate(iterable), key=lambda x: x[1])[0]\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, transform = clip.load(\"ViT-B/32\", device=device)\n","\n","correct = []\n","\n","#define our target classificaitons, you can should experiment with these strings of text as you see fit, though, make sure they are in the same order as your class names above\n","text = clip.tokenize(candidate_captions).to(device)\n","\n","for cls in class_names:\n","    class_correct = []\n","    test_imgs = glob.glob(dataset.location + '/test/' + cls + '/*.jpg')\n","    for img in test_imgs:\n","        #print(img)\n","        image = transform(Image.open(img)).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            image_features = model.encode_image(image)\n","            text_features = model.encode_text(text)\n","            \n","            logits_per_image, logits_per_text = model(image, text)\n","            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","            pred = class_names[argmax(list(probs)[0])]\n","            #print(pred)\n","            if pred == cls:\n","                correct.append(1)\n","                class_correct.append(1)\n","            else:\n","                correct.append(0)\n","                class_correct.append(0)\n","    \n","    # print('accuracy on class ' + cls + ' is :' + str(sum(class_correct)/len(class_correct)))\n","# print('accuracy on all is : ' + str(sum(correct)/len(correct)))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:09<00:00, 38.2MiB/s]\n"]}]}]}